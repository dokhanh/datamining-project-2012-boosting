\documentclass[a4paper,twoside,12pt]{article}
% Alternative Options:
%	Paper Size: a4paper / a5paper / b5paper / letterpaper / legalpaper / executivepaper
% Duplex: oneside / twoside
% Base Font Size: 10pt / 11pt / 12pt


%% Language %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[english,french]{babel}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ae}

\usepackage{lmodern} %Type1-font for non-english texts and characters

\usepackage[top=2.5cm, bottom=2cm, left=2cm, right=2cm]{geometry}
\usepackage{icomma} % Permet l'utilisation de virgule comme séparateur décimal
\usepackage{url} % Package pour ne pas avoir des problèmes avec des URL's
% Utiliser \url{}

%% Packages for Graphics & Figures %%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage[dvips]{graphicx}
%\usepackage{color,psfrag}
\usepackage{graphicx} %%For loading graphic files
%\usepackage{subfig} %%Subfigures inside a figure
%\usepackage{tikz} %%Generate vector graphics from within LaTeX
%\usepackage{tikz-3dplot} %requires 3dplot.sty to be in same directory, or in your LaTeX installation
%\usetikzlibrary{calc} %pour faire les calculs

%\usepackage{epstopdf}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} %Dessine la ligne horizontale

%% Please note:
%% Images can be included using \includegraphics{filename}
%% resp. using the dialog in the Insert menu.
%% 
%% The mode "LaTeX => PDF" allows the following formats:
%%   .jpg  .png  .pdf  .mps
%% 
%% The modes "LaTeX => DVI", "LaTeX => PS" und "LaTeX => PS => PDF"
%% allow the following formats:
%%   .eps  .ps  .bmp  .pict  .pntg


%% Math Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath,amsfonts,amstext,amscd,bezier,amsthm,amssymb}

\newcommand{\vect}[1]{\boldsymbol{#1}}

%\newenvironment{solu}{\noindent {\bf Solution.}\small }{\hfill $\square$ \normalsize \medskip}

%\renewcommand\thesection{\arabic{section}}
%\renewcommand*\thesection{Question \arabic{section}}

\def\MYTITLE{Review on boosting algorithms}

\title{\MYTITLE}
\author{\textsc{Vu} Tuan Hung and \textsc{Do} Quoc Khanh}
\date{\today}

\usepackage{fancyhdr}

\fancyhead[L]{\slshape Tuan Hung VU and Quoc Khanh DO}%\slshape\thepage LE,RO
\fancyhead[R]{\slshape Final report}%{\slshape \leftmark}
%\fancyhead[LO]{ccc}%{\slshape \rightmark}
%\fancyfoot[LO,LE]{}%\slshape Short Course on Asymptotics
\fancyfoot[C]{\thepage}
%\fancyfoot[RO,RE]{}%\slshape 7/15/2002

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\diag}{diag}
\newcommand{\abs}[1]{{\lvert #1 \rvert}}
\newcommand{\norm}[1]{{\lVert #1 \rVert}}

\begin{document}
\maketitle
\pagestyle{fancy}

\section{Introduction}

\section{Two-class classification}
In this first part, we present an overview on boosting methods in the two-class classification framework. From a \textsl{training} set $(\textbf{x}_i, y_i)_{i = 1,...,N}$ in which $\textbf{x}_i \in \mathcal{X}$ and $y_i \in \mathcal{Y}$, we try to construct a function $F: \mathcal{X} \rightarrow \mathcal{Y}$ so that when a new value $\textbf{x}$ is randomly introduced, we have the highest probability to predict correctly the value $y$ corresponding to this value of $\textbf{x}$. Formally, we want to minimize the probability:
\begin{align}
    \mathbb{P}_{(\textbf{x}, y)} \left( y \neq F(\textbf{x})\right) \notag
\end{align}
The variable $\textbf{x}$ is called explanatory variables ($\textbf{x}$ may be multi-variational) and $y$ is called response variable. In the two-class classification framework, $\mathcal{Y} = \{ -1, 1\}$.

\subsection{Boosting and optimization in function space}
We exploit the point of view presented in \cite{trebst} by considering this problem as an estimation and optimization in function space. Indeed, if there exists a function $F^{*}$ which minimizes the above error:
\begin{align}
    F^{*} &= arg\min\limits_{F} \mathbb{P}_{(\textbf{x},y)} (y \neq F(\textbf{x})) \notag \\
    &= arg\min\limits_{F} \mathbb{E}_{(\textbf{x}, y)} \left[ 1(y \neq F(\textbf{x}))\right] \notag
\end{align}
then we are trying to estimate $F^{*}$ by a function $\hat{F}$ through the training set $(\textbf{x}_i, y_i)_{i=1,...,N}$.

\textbf{Base classifiers:} An approach frequently employed by classification algorithms is to suppose $F^{*}$ belongs to a function class parameterized by $\theta \in \Theta$:
\begin{align}
    F^{*} \in \mathcal{Q} = \left\lbrace F(., \theta) \vert \theta \in \Theta\right\rbrace \notag
\end{align}
so that the problem of estimating $F^{*}$ becomes an optimization of the parameters on $\Theta$:
\begin{align}
    \hat{\theta} = arg\min\limits_{\theta \in \Theta} \mathbb{E}_{(\textbf{x}, y)} \left[ 1(y \neq F(\textbf{x}, \theta))\right] \notag
\end{align}
and then we will take $\hat{F} = F(., \hat{\theta}) \in \mathcal{Q}$. For example, with regression tree algorithms, we have:
\begin{align}
    \mathcal{Q} = \left\lbrace F(x, \theta) = \sum\limits_{k=1}^K \lambda_k 1(\textbf{x} \in R_k) \vert (\lambda_1,...,\lambda_K) \in \mathbb{R}^K, (R_1,...,R_K) \in \mathcal{P}_{\mathcal{X}}\right\rbrace \notag
\end{align}
in which $\theta = (\lambda_{1:K}, R_{1:K})$ and $\mathcal{P}_{\mathcal{X}}$ is the set of all partitions of $\mathcal{X}$ into $K$ disjoint subsets by hyperplans which are orthogonal to axes. Similarly for support vector machines, $K$ disjoint subsets $R_1,...,R_K$ are divided by hyperplans in the reproducing kernel Hilbert space of $\mathcal{X}$ corresponding to some kernel.

We can see that a classifier is characterized by its function sub-space $\mathcal{Q}$ and the corresponding parameter space. Having the base classifiers $\mathcal{Q}_{1:M}$ with parameter spaces $\Theta_{1:M}$, instead of considering each of these classifiers separately, boosting methods consider functions of the following additive form:
\begin{align}
    \hat{F} \in \mathcal{F}_{\mathcal{Q}_1,...,\mathcal{Q}_M} = \left\lbrace \sum\limits_{m=1}^M \beta_m F(., \theta_m) \vert \theta_m \in \Theta_m, \forall m = 1,...,M\right\rbrace \notag
\end{align}
so that the optimization problem becomes:
\begin{align}
    \left\lbrace \hat{\beta}_{1:M}, \hat{\theta}_{1:M}\right\rbrace = 
\end{align}

The paper explains boosting . Advantage

\subsection{One-degree optimization}

\subsection{Two-degree optimization}

\section{Multi-class classification and some generalizations}

\subsection{A traditional approach}

\subsection{Some generalization of two-class algorithms}

\subsection{Other generalizations}

\section{Experiments}

\subsection{Experiments with simulated data}

\subsection{Experiments with real data}

\section{Conclusion}

\begin{thebibliography}{9999}%\enlargethispage{\baselineskip}
\bibitem[1]{boost}Friedman, J., Hastie, T. \& Tibshirani, R. \textsl{Additive Logistic Regression: a Statistical View of Boosting}, 2000.

\bibitem[2]{trebst}Friedman, J. \textsl{Greedy Function Approximation: A Gradient Boosting Machine}, IMS 1999 Reitz Lecture, 2001.

\bibitem[3]{SchaAndSin1998}Schapire, R.E. \& Singer, Y. \textsl{Improved Boosting Algorithms: Using Confidence-rated Predictions}, 1998.

\end{thebibliography}

\end{document}