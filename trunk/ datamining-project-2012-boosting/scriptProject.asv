%% créer le data set
alpha = 0.2; % erreurs aléatoires
N = 500;
d = 2; % nombre de variables explicatives
[xtrain,ytrain] = rexemple(alpha, N, d);

%% bayes classifier
[xtest, ytest] = rexemple(alpha, 10000, d);
ytrained = 2*(2*xtest(:, 1) + xtest(:, 2) > 1.5)-1;
bayesError = sum(ytrained ~= ytest)/10000

%% Discrete AdaBoost
w = (1/N)*ones(N,1);
M = 10; % nombre d'itérations
trees = cell(M,1);
err = zeros(M,1); % error at each iteration
C = zeros(M,1);
for m = 1:M
    t = classregtree(xtrain,ytrain,'minparent',N,'weights',w,'method','classification');
    trees{m} = t;
%     ytrained = eval(t,xtrain);
%     err(m) = 0;
%     for i = 1:N
%         err(m) = err(m)+(str2double(ytrained{i}) ~= ytrain(i))*w(i);
%     end
    err(m) = test(t,'crossvalidate',xtrain,ytrain,)
    C(m) = log((1 - err(m))/err(m));
    for i = 1:N
        w(i) = w(i)*exp(C(m)*(str2double(ytrained{i}) ~= ytrain(i)));
    end
    w = w/sum(w);
end

%% Output
n = 10000;
[xtest, ytest] = rexemple(alpha, n, d);
res = zeros(n, M);
for m = 1:M
    l = eval(trees{m}, xtest);
    for i = 1:n
        res(i, m) = str2double(l{i});
    end
end
resSynt = zeros(n, M);
for m = 1:M
    resSynt(:, m) = 2*(res(:, 1:m)*C(1:m, 1) > 0) - 1;
end
errSynt = zeros(M, 1);
for m = 1:M
    errSynt(m) = sum(resSynt(:, m) ~= ytest)/n;
end