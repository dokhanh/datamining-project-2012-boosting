\relax 
\catcode`:\active
\catcode`;\active
\catcode`!\active
\catcode`?\active
\citation{boost}
\citation{trebst}
\citation{SchaAndSin1998}
\citation{trebst}
\select@language{french}
\@writefile{toc}{\select@language{french}}
\@writefile{lof}{\select@language{french}}
\@writefile{lot}{\select@language{french}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Two-class classification}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Boosting and optimization in function space}{1}}
\newlabel{boo_fun_sp}{{2.1}{1}}
\citation{boost}
\citation{trebst}
\newlabel{opt_add}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}$L(y, F(\textbf  {x})) = e^{-yF(\textbf  {x})}$}{3}}
\newlabel{exp_loss_part}{{2.2}{3}}
\newlabel{exp_loss_approcher}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Error rates corresponding to \textsl  {Discrete AdaBoost}, \textsl  {Real AdaBoost} and \textsl  {Gentle AdaBoost}.}}{4}}
\newlabel{loss_exp}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Least-Square Boost}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Least-Absolute-Deviation Boost}{7}}
\newlabel{LAD}{{2.4}{7}}
\newlabel{sec:LAD_treeboost}{{2.4}{7}}
\newlabel{opt_LAD_local}{{3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}M-regression loss function}{7}}
\newlabel{sec:m_treeboost}{{2.5}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Negative Binomial log-likelihood loss function}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of the performance of population and non-population version of \textsl  {Logit Boost} algorithm on simulated data.}}{9}}
\newlabel{comp_pop_nonPop}{{2}{9}}
\citation{SchaAndSin1998}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}$L(y, F(\textbf  {x})) = \qopname  \relax o{log}(1+e^{-2yF(\textbf  {x})})$}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Multi-class classification and some generalizations}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}A traditional approach}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of the performances of \textsl  {AdaBoost.MH} using $6$ two-class boosting methods, experiments on simulated data with $0 \%$ Bayes error.}}{11}}
\newlabel{comp_MH}{{3}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison of the performances of \textsl  {AdaBoost.MH} using $6$ two-class boosting methods, experiments on simulated data with $5 \%$ Bayes error.}}{11}}
\newlabel{comp_MH1}{{4}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of the performances of \textsl  {AdaBoost.MH} using $6$ two-class boosting methods, experiments on simulated data with $10 \%$ Bayes error.}}{12}}
\newlabel{comp_MH2}{{5}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Some generalization of two-class algorithms}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Multi-class classification on simulated data, $J = 4$, Bayes error $0\%$.}}{13}}
\newlabel{multi_comp0}{{6}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Multi-class classification on simulated data, $J = 4$, Bayes error $5\%$.}}{13}}
\newlabel{multi_comp1}{{7}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Multi-class classification on simulated data, $J = 4$, Bayes error $10\%$.}}{14}}
\newlabel{multi_comp2}{{8}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Multi-class classification on simulated data, $J = 6$, Bayes error $0\%$.}}{14}}
\newlabel{multi_comp3}{{9}{14}}
\citation{boost}
\citation{trebst}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experiments with simulated data}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Simulated Training Dataset}}{16}}
\newlabel{fig:simulated_data}{{10}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Comparison of Boosting Algorithms with Binary Classification}}{16}}
\newlabel{fig:bin_class_comparison}{{11}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Comparison of Boosting Algorithms with Binary Classification - With 5\% noise}}{17}}
\newlabel{fig:bin_class_comparison_noise_5per}{{12}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Comparison of Boosting Algorithms with Binary Classification - With 10\% noise}}{17}}
\newlabel{fig:bin_class_comparison_noise_10per}{{13}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Experiments with real data}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Comparison of Boosting Algorithms with UCI Wisconsin Breast Cancer Dataset}}{18}}
\newlabel{fig:wbc}{{14}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Comparison of Multi-class Boosting Algorithms with UCI Wine Quality Dataset}}{18}}
\newlabel{fig:wine_quality}{{15}{18}}
\citation{SchaAndSin1998}
\bibcite{boost}{1}
\bibcite{trebst}{2}
\bibcite{SchaAndSin1998}{3}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary on boosting algorithms.}}{19}}
\newlabel{sum_tab}{{1}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{19}}
